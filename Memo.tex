

\documentclass[a4paper,12pt]{article}


%\usepackage[spanish,activeacute]{babel} 
\usepackage[utf8]{inputenc} 
\usepackage{latexsym,amsfonts,amsmath,mathrsfs,amssymb}
\usepackage{verbatim}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{makeidx}        
\usepackage{multicol}        



\author{Pedro Parrado Rodríguez\\* {\small PhD}}


\title{{\bf{Memory}}} 

% La fecha de entrega del informe y el número de la versión (1ª, 2ª, 3ª,..)
\date{{\scriptsize }}
\oddsidemargin0cm
\textwidth16cm


% Comenzamos el documento 
%jbg%\usepackage{Sweave}
\begin{document}

% Pone el título que hemos introducido arriba
\maketitle

\tableofcontents
\newpage


%\chapter{First Year}
\section{Warm up}
\subsection{Lookup tables}

A lookup table is the simplest decoder. It is a table in which we have the correction to apply for every syndrome. 

To generate the lookup table for the simplest codes, I have explored all possible combinations of up to 5 errors. Then for each error combination, I find the syndrome that it generates, and I add it to the lookuptable if there is no other error configuration for that syndrome with less errors (meaning, we only keep the error configurations with the minimum weight).

Lookup tables are probably the fastest decoders, as they take just one step to decode a syndrome. The problem comes at generating them, as the size of the table grows exponentially with the number of stabilizers ($2^N$), and both filling that table and storing it becomes impossible after increasing the system size just a bit. 

\subsection{MonteCarlo simulations with Lookup tables}

To check the efficiency of the lookuptables, I have run some MonteCarlo simulations. The simulation works with the following steps:
\begin{enumerate}
\item First, we generate an error configuration by throwing errors at each qubit with probability $p$. For each error appearing in the code, we apply a pauli operator $X$, $Y$ or $Z$ with probability $1/3$.
\item Second, we measure the syndrome for the system with that error configuration.
\item Then, we apply the correction from the lookuptable for that syndrome.
\item Finally, we check the combination of the error and our correction, to see if there is a logical error or not in the final result.
\end{enumerate}

From each of those simulations we get either a 1 in case of logical error, or a 0 in case of a proper correction. We then repeat this simulation several times (of the order of $10^4$ at least) to find the probability of logical error, and we compute that probability for different values of $p$ (the probability of an error in a single qubit) to find the behavior of the probability of logical error, and to find the threshold (when $p_{Logical\quad error}=p_{error\quad in \quad a \quad qubit}$).
\newpage

\section{Surface Code}

The surface code is an algorithm to encode quantum information in several physical qubits. It is a stabilizer code, in which we have the qubits arranged in the edges of a square lattice, the $Z$ stabilizers in the faces (acting over the 4 qubits on the sides), and the $X$ stabilizers in the sites (acting also over the 4 qubits corresponding to the edges that cross that site). 

The surface code corresponds to the family of topological codes. It can be encoded as a toric code, if we are allowed to introduce periodic boundary conditions. In that case, we encode 2 logical qubits, and its logical operators are vertical and horizontal chains of pauli operators. Otherwise, we can use rough and smooth boundaries to use the code without periodic boundary conditions.  In that case, we only encode one logical qubit, and the logical operators are also chains of pauli operators, that go from smooth - smooth borders for the $X_L$ and from rough to rough borders for the $Z_L$.

As the stabilizers are either formed of only $X$ or $Z$ operators, it is a CSS code, so we can focus on just one of the 2 for the rest of the chapter.

\subsection{MLE Decoder}

In the surface code, an error produces two excitations of stabilizer measurements, so they are easy to detect. However, when we have a chain of errors, that only excites 2 stabilizers, one at the beginning and one at the end of the chain. That makes it more difficult to find the error, when we have several excitations in the code.  The good thing is that error chains of the same topological class are equivalent up to stabilizers, so the goal of a decoder would be to find the most probable homology class for a given syndrome. That problem, though, is too hard to solve, but we can still do a very good approximation by finding the Most Likely Error (MLE Decoder).

As the error chains generate  pairs of excitations, the way the decoder works is by finding the minimum weight perfect matching of the syndrome. To do that, we use the Blossom algorithm, from a C implementation made by Kolmogorov.


\subsection{MonteCarlo simulation}

Once we have the decoder working, our goal is to find the threshold of the surface code. This means, finding the critical value of $p_c$ such that, below that value, we can improve the precision of the code as much as we want by increasing the code size.

The way we are going to do this is by Monte-Carlo simulations.  By simulating error configurations for different values of $p$, we can compute an approximation to the probability of logical error, counting the percentage of cases in which after the correction there is a logical error in the code. Having this tool, we can compute the logical error probability around the critical point for different system sizes. Then, we can do a multiparameter fit to find the value of the critical point:

\begin{figure}[ht!]
\begin{center}
%\includegraphics[height=90mm]{thresholdSurfCode.png}
\end{center}
\end{figure}


\subsection{Measurement errors Decoder}
\subsection{MonteCarlo simulation}
\newpage

\section{Color Code}
\subsection{Rescaling decoder}
\subsubsection{Outline of the decoder}

\begin{enumerate}
\item Read the Syndrome from the errors in the qubits.
\item Assign an initial splitting (or initial split probabilities).
\item Update the splittings (or the split probabilities) until convergence.
\item Decode the cells independently according to the splitting from the previous step.
\item Create the new rescaled code, and assign to each logical qubit the probability of a logical error in the cell.
\item Apply the decoder again, until the size of the code is small enough to apply a complete lookuptable.
\item Apply the corrections of the higher levels to the lower ones.


\end{enumerate}

\subsubsection{Hard splitting method}
This algorithm for splitting is based on changing each individual splitting to a better one asuming the others are constant, until convergence. Therefore, the core of the algorithm is to compute the probability of a given splitting:

\begin{equation}
p(s_0^u|s_1s_2s_1's_2')=\frac{p(s_0^u|s_1s_2)p(s_0^l|s_1's_2')}{p(s_0^u|s_1s_2)p(s_0^l|s_1's_2')+[1-p(s_0^u|s_1s_2)][1-p(s_0^l|s_1's_2']},
\label{condprobsplit}
\end{equation}

where the conditional probabilities are:

\begin{equation}
p(s_0^u|s_1s_2)=\frac{p(s_0^us_1s_2)}{p(s_0^u=1,s_1s_2)+p(s_0^u=0,s_1s_2)},
\end{equation}

where the probability $p(s_0^us_1s_2)$ is computed as the sum of the probabilities of all the error configurations which are compatible with the syndrome.

So, using equation \ref{condprobsplit} for a given stabilizer in the boundary of 2 cells, we can choose which splitting is the most probable, given the rest. Now the idea is to repeat this selection process for every slitting several times until we reach convergence, and that brings us to the next problem, which is how to update those splittings in a way that allows us to get to the best splitting.

Before going into the ways of choosing the updates, we are going to define some energy-like function to characterize the likelihood of a splitting. This energy should be minimum for the best splitting, and we define it as:
\begin{equation}
E(\{ split\})=-\sum_{cells} \log p(s_0s_1s_2).
\label{energyofsplitting} 
\end{equation}
Which is related to the probability of the error configurations that are compatible with the particular choice of the splitting. We will use this function to characterize the improvement in the choice of a splitting. These are some of those methods:

\begin{enumerate}
\item The first method would be to simply update all of the splittings in a fixed order. That can lead to cycles, due to the way of choosing the splittings.
\item The second way would be to update first the horizontal borders, then the verticals, and then the diagonal (the actual order of these 3 does not matter, just the fact that we update them in different steps). This can also lead to cycles, but we can parallelize this method as the updates of these 3 types of borders are independent.

\item The third way is to choose the next border to update at random. This solves the problem of the cycles, but also has the same problem of the other 2 methods, we cannot be sure that we reach the real minimum instead of a local minimum.
\end{enumerate}

All of these methods have a problem that comes from the fact that, in each update of a single split, we always choose the one with the highest probability. That prevents us from exploring the whole space of options, and therefore can lead to the algorithm getting stuck in a local minimum of energy, because changing any single split would increase the energy, even though changing 2 at the same time could improve the energy. 

One possible improvement to this method would be to allow a split to swap to a least probable option, in a thermal-montecarlo-like simulation. We could also swap clusters of splits at the same time. A different way of doing it is the Soft-splitting method.

\subsubsection{Soft splitting method}

This algorithm for splitting, instead of choosing fixed values for a set of splittings and explore the space by flipping splittings in the set, assigns a probability for each splitting. That means, we have now a function $p(s_0^u)$ for each splitting in the code instead of a value of 0 or 1. The key idea is that we now update those probabilities $p(s_0^u)$ according to the probabilities of the error configurations compatible with every possible splitting choice of the neighbouring splittings ($s_1,s_2,s_1',s_2'$), which means that we are now moving in a continuous way from one splitting choice to the other. 

The way to update a particular splitting is by using the following equations:
\begin{equation}
p(s_0^u)_{t+1}= \frac{p(s_0^u)p(s_0^l)}{p(s_0^u)p(s_0^l)+[1-p(s_0^u)][1-p(s_0^l)]}
\end{equation}
where those intermediate probabilities are computed by:
\begin{equation}
p(s_0^u)=\sum \frac{p(s_0^us_1s_2)p(s_1)p(s_2)}{p(s_0^u=1s_1s_2)+p(s_0^u=0s_1s_2)},
\end{equation}
\begin{equation}
p(s_0^l)=\sum \frac{p(s_0^ls_1's_2')p(s_1')p(s_2')}{p(s_0^l=1s_1's_2')+p(s_0^l=0s_1's_2')}.
\end{equation}
The probabilities $p(s_i)$ are the current values of the probability of each splitting, which are also to be updated in the next steps. The probabilities $p(s_0s_1s_2)$ are comp
uted as before, by the probabilities of every different error configuration which is compatible with that choice of splittings.

As with the soft decoder, we can use different methods to choose the order of the updates of the splittings.


Once we have a good splitting, we can proceed to decode the cells independently of each other, by applying a decoder.

\subsubsection{Cell Decoder}

The decoding of a single cell of four qubits is simple. We have 3 values for the syndromes in the boundaries with the other cells $s_0$, $s_1$ and $s_2$; and 4 qubits. That means, we have a total of $2^3$ possible syndrome measurements, and $2^4$ possible error configurations, 2 per syndrome measurement. We can build a complete lookuptable with the possible configurations of errors.


\begin{figure}[ht!]
\begin{center}
%\includegraphics[height=30mm]{4qcell.png}
\caption{4 qubit cell, and the logical operator of that cell (shaded cells).}

\end{center}
\end{figure}
\begin{table}
\begin{center}
\begin{tabular}{|ccc|c|c|}
\hline
$s_0$&$s_1$	&$s_2$&Correction&Correction + Log. Operator\\
\hline
+	&+	&+	&-	&0,2,3	\\
+	&+	&-	&0,1	&1,2,3	\\
+	&-	&+	&1,2	&0,3		\\
+	&-	&-	&3	&0,2		\\
-	&+	&+	&1,3	&0,1,2	\\
-	&+	&-	&2	&0,3		\\
-	&-	&+	&0	&2,3		\\
-	&-	&-	&1	&0,1,2,3	\\
\hline
%\caption{Lookup table}
\end{tabular}
\end{center}
\end{table}

We can use the lookup table to compute the probability of each of the 2 possible corrections, knowing the probability of error of each individual qubit (each qubit contributes with its $p_{err}$, or $1-p_{err}$).


After choosing the best option, the error probability of the renormalized cell is the probability of choosing the other option. In other words, the probability of applying also the logical operator ($\hat{X}$), which will be the operation that we will apply if we decide to apply a correction in the next step:
\begin{equation}
p_{err}=\frac{p(e+\hat{X})}{p(e)+p(e+\hat{X})}
\end{equation}

% IMAGEN

%\begin{figure}[ht!]
%\begin{center}
%\includegraphics[height=90mm]{IMAGEN.png}
%\end{center}
%\end{figure}
%



%  TABLA

%\begin{table}[h]
%\begin{center}
%\begin{tabular}{|c|c|c|c|c|c|c|}
%\hline
%$		$&$		$&$		$&$		$&$		$&$		$&$		$\\
%\hline
%\end{tabular}
%\end{center}
%\end{table}





\end{document}
