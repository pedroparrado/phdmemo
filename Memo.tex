

\documentclass[a4paper,12pt]{article}


%\usepackage[spanish,activeacute]{babel} 
\usepackage[utf8]{inputenc} 
\usepackage{latexsym,amsfonts,amsmath,mathrsfs,amssymb}
\usepackage{verbatim}
\usepackage{fancyvrb}
\usepackage{graphicx}
\usepackage{makeidx}        
\usepackage{multicol}        



\author{Pedro Parrado Rodríguez\\* {\small PhD}}


\title{{\bf{Memory}}} 

% La fecha de entrega del informe y el número de la versión (1ª, 2ª, 3ª,..)
\date{{\scriptsize }}
\oddsidemargin0cm
\textwidth16cm


% Comenzamos el documento 
%jbg%\usepackage{Sweave}
\begin{document}

% Pone el título que hemos introducido arriba
\maketitle

\tableofcontents
\newpage


%\chapter{First Year}
\section{Warm up}
\subsection{Lookup tables}

A lookup table is the simplest decoder. It is a table in which we have the correction to apply for every syndrome. 

To generate the lookup table for the simplest codes, I have explored all possible combinations of up to 5 errors. Then for each error combination, I find the syndrome that it generates, and I add it to the lookuptable if there is no other error configuration for that syndrome with less errors (meaning, we only keep the error configurations with the minimum weight).

Lookup tables are probably the fastest decoders, as they take just one step to decode a syndrome. The problem comes at generating them, as the size of the table grows exponentially with the number of stabilizers ($2^N$), and both filling that table and storing it becomes impossible after increasing the system size just a bit. 

\subsection{MonteCarlo simulations with Lookup tables}

To check the efficiency of the lookuptables, I have run some MonteCarlo simulations. The simulation works with the following steps:
\begin{enumerate}
\item First, we generate an error configuration by throwing errors at each qubit with probability $p$. For each error appearing in the code, we apply a pauli operator $X$, $Y$ or $Z$ with probability $1/3$.
\item Second, we measure the syndrome for the system with that error configuration.
\item Then, we apply the correction from the lookuptable for that syndrome.
\item Finally, we check the combination of the error and our correction, to see if there is a logical error or not in the final result.
\end{enumerate}

From each of those simulations we get either a 1 in case of logical error, or a 0 in case of a proper correction. We then repeat this simulation several times (of the order of $10^4$ at least) to find the probability of logical error, and we compute that probability for different values of $p$ (the probability of an error in a single qubit) to find the behavior of the probability of logical error, and to find the threshold (when $p_{Logical\quad error}=p_{error\quad in \quad a \quad qubit}$).
\newpage

\section{Surface Code}

The surface code is an algorithm to encode quantum information in several physical qubits. It is a stabilizer code, in which we have the qubits arranged in the edges of a square lattice, the $Z$ stabilizers in the faces (acting over the 4 qubits on the sides), and the $X$ stabilizers in the sites (acting also over the 4 qubits corresponding to the edges that cross that site). 

The surface code corresponds to the family of topological codes. It can be encoded as a toric code, if we are allowed to introduce periodic boundary conditions. In that case, we encode 2 logical qubits, and its logical operators are vertical and horizontal chains of pauli operators. Otherwise, we can use rough and smooth boundaries to use the code without periodic boundary conditions.  In that case, we only encode one logical qubit, and the logical operators are also chains of pauli operators, that go from smooth - smooth borders for the $X_L$ and from rough to rough borders for the $Z_L$.

As the stabilizers are either formed of only $X$ or $Z$ operators, it is a CSS code, so we can focus on just one of the 2 for the rest of the chapter.

\subsection{MLE Decoder}

In the surface code, an error produces two excitations of stabilizer measurements, so they are easy to detect. However, when we have a chain of errors, that only excites 2 stabilizers, one at the beginning and one at the end of the chain. That makes it more difficult to find the error, when we have several excitations in the code.  The good thing is that error chains of the same topological class are equivalent up to stabilizers, so the goal of a decoder would be to find the most probable homology class for a given syndrome. That problem, though, is too hard to solve, but we can still do a very good approximation by finding the Most Likely Error (MLE Decoder).

As the error chains generate  pairs of excitations, the way the decoder works is by finding the minimum weight perfect matching of the syndrome. To do that, we use the Blossom algorithm, from a C implementation made by Kolmogorov.


\subsection{MonteCarlo simulation}

Once we have the decoder working, our goal is to find the threshold of the surface code. This means, finding the critical value of $p_c$ such that, below that value, we can improve the precision of the code as much as we want by increasing the code size.

The way we are going to do this is by Monte-Carlo simulations.  By simulating error configurations for different values of $p$, we can compute an approximation to the probability of logical error, counting the percentage of cases in which after the correction there is a logical error in the code. Having this tool, we can compute the logical error probability around the critical point for different system sizes. Then, we can do a multiparameter fit to find the value of the critical point:

\begin{figure}[ht!]
\begin{center}
%\includegraphics[height=90mm]{thresholdSurfCode.png}
\end{center}
\end{figure}


\subsection{Measurement errors Decoder}
\subsection{MonteCarlo simulation}
\newpage

\section{Color Code}
\subsection{Rescaling decoder}
\subsubsection{Outline of the decoder}

\begin{enumerate}
\item Read the Syndrome from the errors in the qubits.
\item Assign an initial splitting (or initial split probabilities).
\item Update the splittings (or the split probabilities) until convergence.
\item Decode the cells independently according to the splitting from the previous step.
\item Create the new rescaled code, and assign to each logical qubit the probability of a logical error in the cell.
\item Apply the decoder again, until the size of the code is small enough to apply a complete lookuptable.
\item Apply the corrections of the higher levels to the lower ones.


\end{enumerate}

\subsubsection{Hard splitting method}
This algorithm for splitting is based on changing each individual splitting to a better one asuming the others are constant, until convergence. Therefore, the core of the algorithm is to compute the probability of a given splitting:

\begin{equation}
p(s_0^u|s_1s_2s_1's_2')=\frac{p(s_0^u|s_1s_2)p(s_0^l|s_1's_2')}{p(s_0^u|s_1s_2)p(s_0^l|s_1's_2')+[1-p(s_0^u|s_1s_2)][1-p(s_0^l|s_1's_2']},
\label{condprobsplit}
\end{equation}

where the conditional probabilities are:

\begin{equation}
p(s_0^u|s_1s_2)=\frac{p(s_0^us_1s_2)}{p(s_0^u=1,s_1s_2)+p(s_0^u=0,s_1s_2)},
\end{equation}

where the probability $p(s_0^us_1s_2)$ is computed as the sum of the probabilities of all the error configurations which are compatible with the syndrome.

So, using equation \ref{condprobsplit} for a given stabilizer in the boundary of 2 cells, we can choose which splitting is the most probable, given the rest. Now the idea is to repeat this selection process for every slitting several times until we reach convergence, and that brings us to the next problem, which is how to update those splittings in a way that allows us to get to the best splitting.

Before going into the ways of choosing the updates, we are going to define some energy-like function to characterize the likelihood of a splitting. This energy should be minimum for the best splitting, and we define it as:
\begin{equation}
E(\{ split\})=-\sum_{cells} \log p(s_0s_1s_2).
\label{energyofsplitting} 
\end{equation}
Which is related to the probability of the error configurations that are compatible with the particular choice of the splitting. We will use this function to characterize the improvement in the choice of a splitting. These are some of those methods:

\begin{enumerate}
\item The first method would be to simply update all of the splittings in a fixed order. That can lead to cycles, due to the way of choosing the splittings.
\item The second way would be to update first the horizontal borders, then the verticals, and then the diagonal (the actual order of these 3 does not matter, just the fact that we update them in different steps). This can also lead to cycles, but we can parallelize this method as the updates of these 3 types of borders are independent.

\item The third way is to choose the next border to update at random. This solves the problem of the cycles, but also has the same problem of the other 2 methods, we cannot be sure that we reach the real minimum instead of a local minimum.
\end{enumerate}

All of these methods have a problem that comes from the fact that, in each update of a single split, we always choose the one with the highest probability. That prevents us from exploring the whole space of options, and therefore can lead to the algorithm getting stuck in a local minimum of energy, because changing any single split would increase the energy, even though changing 2 at the same time could improve the energy. 

One possible improvement to this method would be to allow a split to swap to a least probable option, in a thermal-montecarlo-like simulation. We could also swap clusters of splits at the same time. A different way of doing it is the Soft-splitting method.

\subsubsection{Soft splitting method}

This algorithm for splitting, instead of choosing fixed values for a set of splittings and explore the space by flipping splittings in the set, assigns a probability for each splitting. That means, we have now a function $p(s_0^u)$ for each splitting in the code instead of a value of 0 or 1. The key idea is that we now update those probabilities $p(s_0^u)$ according to the probabilities of the error configurations compatible with every possible splitting choice of the neighbouring splittings ($s_1,s_2,s_1',s_2'$), which means that we are now moving in a continuous way from one splitting choice to the other. 

The way to update a particular splitting is by using the following equations:
\begin{equation}
p(s_0^u)_{t+1}= \frac{p(s_0^u)p(s_0^l)}{p(s_0^u)p(s_0^l)+[1-p(s_0^u)][1-p(s_0^l)]}
\end{equation}
where those intermediate probabilities are computed by:
\begin{equation}
p(s_0^u)=\sum \frac{p(s_0^us_1s_2)p(s_1)p(s_2)}{p(s_0^u=1s_1s_2)+p(s_0^u=0s_1s_2)},
\end{equation}
\begin{equation}
p(s_0^l)=\sum \frac{p(s_0^ls_1's_2')p(s_1')p(s_2')}{p(s_0^l=1s_1's_2')+p(s_0^l=0s_1's_2')}.
\end{equation}
The probabilities $p(s_i)$ are the current values of the probability of each splitting, which are also to be updated in the next steps. The probabilities $p(s_0s_1s_2)$ are comp
uted as before, by the probabilities of every different error configuration which is compatible with that choice of splittings.

As with the soft decoder, we can use different methods to choose the order of the updates of the splittings.


Once we have a good splitting, we can proceed to decode the cells independently of each other, by applying a decoder.

\subsubsection{Cell Decoder}

The decoding of a single cell of four qubits is simple. We have 3 values for the syndromes in the boundaries with the other cells $s_0$, $s_1$ and $s_2$; and 4 qubits. That means, we have a total of $2^3$ possible syndrome measurements, and $2^4$ possible error configurations, 2 per syndrome measurement. We can build a complete lookuptable with the possible configurations of errors.


\begin{figure}[ht!]
\begin{center}
%\includegraphics[height=30mm]{4qcell.png}
\caption{4 qubit cell, and the logical operator of that cell (shaded cells).}

\end{center}
\end{figure}
\begin{table}
\begin{center}
\begin{tabular}{|ccc|c|c|}

\hline
$s_0$&$s_1$	&$s_2$&Correction&Correction + Log. Operator\\
\hline
+	&+	&+	&-	&0,2,3	\\
+	&+	&-	&0,1	&1,2,3	\\
+	&-	&+	&1,2	&0,3		\\
+	&-	&-	&3	&0,2		\\
-	&+	&+	&1,3	&0,1,2	\\
-	&+	&-	&2	&0,3		\\
-	&-	&+	&0	&2,3		\\
-	&-	&-	&1	&0,1,2,3	\\
\hline
%\caption{Lookup table}
\end{tabular}
\end{center}
\end{table}

We can use the lookup table to compute the probability of each of the 2 possible corrections, knowing the probability of error of each individual qubit (each qubit contributes with its $p_{err}$, or $1-p_{err}$).


After choosing the best option, the error probability of the renormalized cell is the probability of choosing the other option. In other words, the probability of applying also the logical operator ($\hat{X}$), which will be the operation that we will apply if we decide to apply a correction in the next step:
\begin{equation}
p_{err}=\frac{p(e+\hat{X})}{p(e)+p(e+\hat{X})}
\end{equation}

\subsubsection{Decoder for the 18 qubit case}

After each rescaling, we reduce the number of qubits by a factor of 4. In each of those processes, we simply decode groups of 4 cells and generate a reduced version of the codes. This process goes on until we reach a system size small enough to decode the entire code. That point is the 18 qubit case, and in this section we are going to see a couple of ways of decoding that final state.\\

The decoding process consists in using a lookup table. For the syndrome that we find in the code, we check in the table the possible error configurations consistent with that error configurations. Then, from there, we decide which correction we should apply. If the probability of error of every qubit was the same, we could have simply one option in the lookuptable for each syndrome, and then apply that one. However, as the rescaling procedure assigns different error probabilities to each cell after the rescaling, we need to take that into account. Therefore, this decoding procedure consists of 2 steps: First, the generation of the lookuptable, which only needs to be made once before using any decoder, and can be stored in a file; and second, the selection of the appropiate correction from the lookuptable for a given syndrome and set of error probabilities of the qubits.\\

The generation of the lookuptable is simple, but before explaining how it is generated, let us clarify how it is supposed to work. The table is a list of sets: the index of the table corresponds to the syndrome, and the set which is stored in that position of the table is the set of error configurations consistent with that particular syndrome. To put an example, lets say that, from the 9 stabilizers $S_i,$ $i=1,...9$, we have measured an excitation on $S_7$. We can write that state in binary, by assigning a 0 to nonexcited stabilizers and a 1 to excited ones: 
$$S_0S_1S_2S_3S_4S_5S_6S_7S_8=000000010$$

We can read that as a number in binary, which corresponds to number 3. Therefore, if we measure that syndrome, we will check the position 3 in our lookuptable. And, in that position of the list, we will find a set of the possible error configurations consistent with that syndrome in particular. To indentify the error configurations, we will also write them in binary (1's for the qubits with errors, and 0's for the rest), such that an error configuration consisting in errors in the qubits 2,3 and 5 would be written as:
$$E=001101000000000000$$


Now that we know how to read the lookuptable, let us see how to generate it: we generate all $2^18$ possible error configurations, by simply generating all binary numbers from 0 to $2^18$. Then, for each error configuration, we compute the syndrome that will be obtained from that error, which leads us to a position on the list (our lookup table, which starts being empty). We add the current error configuration to the set in that position in the lookuptable, and we keep exploring error configurations. After having explored all error configurations, we have the entire lookuptable ready to use. \\

It is important to note that, in the $2^9$ (9 is the number of stabilizers in the 18 qubit code) positions in the lookuptable, the distribution of errors in the sets is not even. That is due to the fact that from the 9 stabilizers, only 7 are independent.  Therefore, we will have some positions in the table for which we will have no error configurations, because those syndromes are incompatible with the model. And, for the rest of them (we will have exactly $2^7$ positions with nonempty sets), we will have $2^11$ different error configurations in each set. The reason for that is that we can always apply any combination of the stabilizers and the logical operators to the system and we will end up with the same syndrome. That makes $2^7\cdot 2^4=2^11$ combinations for each syndrome.\\

Once we have the lookuptable, we need to be able to extract from it the correction that we should apply. The simplest way to do this is to find the error configuration with the maximum error probability. As we have the error probability for each qubit, what we do is to compute for each of the $2^11$ error configurations that we find in the lookuptable for the given syndrome the correspondent probability. That probability is the product of a factor of either $p_i$ or $(1-p_i)$ for each qubit if it has an error or not. Then, we simply need to find the maximum of that error probability, and apply that correction. This would be a MLE decoder (maximum likelihood error).\\

However, our goal with the error correction procedure is not only to return the system to the codespace, but also to correct the logical errors. Therefore, we could try to improve the procedure by computing, instead of the error configuration with the maximum likelyhood, the homology class with the most likelyhood. That means, computing the likelyhood of any case with no logical operators applied, with one of them, 2 of them, etc, and then apply the correction according to which of those scenarios is the most probable.

For this process of error correction, we need a new and improved lookuptable, in which for each syndrome we will have not one, but $2^4$ different sets (the different combinations of the 4 logical operators). Each of those sets will correspond to one of the homology classes, and the decoder will have to compute the probability of each of those clases by adding the probabilities of each of those configurations. Then, we will apply the best correction from the best homology class. Let us see how to generate that improved lookuptable:

First of all, we will start with the information already available from the first lookuptable. For each non-empty position in the first lookuptable, we will pick just one of the error configurations (the rest are equivalent, either up to stabilizers or by applying some set of logical operators). That configuration, $E_0$, will be our starting point to fill the $2^4$ sets in that position of the improved lookuptable. By applying the $2^7$ possible combinations of stabilizers, we will fill the first set. Then, we will apply one of the $2^4$ logical operator combinations (For example, $X_{L1}$) to $E_0$, and fill the set by applying again the $2^7$ combinations of stabilizers to that new starting combination $\{X_L\}E_0$. After applying all $2^4$ combinations of Logical operators, all the sets for that syndrome will be filled, and we can continue with the next non-empty position in the lookup table.\\

Although theoretically this second method should give a higher probability of success (understood as not having applyed any logical operator after the error correction), if the error probability of all qubits are equal, the monte-carlo simulations reveal an equal probability of success for both methods.

% IMAGEN

%\begin{figure}[ht!]
%\begin{center}
%\includegraphics[height=90mm]{IMAGEN.png}
%\end{center}
%\end{figure}
%



%  TABLA

%\begin{table}[h]
%\begin{center}
%\begin{tabular}{|c|c|c|c|c|c|c|}
%\hline
%$		$&$		$&$		$&$		$&$		$&$		$&$		$\\
%\hline
%\end{tabular}
%\end{center}
%\end{table}





\end{document}
